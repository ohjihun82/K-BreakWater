{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# \"DETECTRON2 USING SEGMENTATION\"\n",
        "\n",
        "> \"DETECTRON2 USING SEGMENTATION\"\n",
        "\n",
        "- toc:true\n",
        "- branch: master\n",
        "- badges: true\n",
        "- comments: true\n",
        "- author: HyunsooKim\n",
        "- categories: [jupyter, python]"
      ],
      "metadata": {
        "id": "oZkDlgjU1re8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DETECTRON2를 이용한 SEGMENTATION"
      ],
      "metadata": {
        "id": "X7k54O2QibYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "사전 작업 \n",
        "\n",
        "segmentation를 하기 위해 labelme를 이용하여 이미지 파일의 물 객체를 폴리곤 형태로 지정한다.\n",
        "\n",
        "detection를 하기 위해 LabelImg를 이용해서 물의 객체를 지정한다.\n"
      ],
      "metadata": {
        "id": "I-hUcgq2hY-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "사용자의 구글 드라이브의 파일을 불러오기 위해서 연결"
      ],
      "metadata": {
        "id": "OOMvdfUEhLxT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMVD3IvYyG-g"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DE_geIiywXz"
      },
      "source": [
        "Detectron2를 colab에 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYEYQoPzypil"
      },
      "outputs": [],
      "source": [
        "!pip install pyyaml==5.1\n",
        "!pip install torch==1.8.0+cu101 torchvision==0.9.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# pytorch를 최신 버전인 1.9가 아닌 1.8를 사용하는 이유는 아직 detectron2에 pytorch 1.9 패키지가 출시되지 않았기 떄문이다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQ7xtG6VzWY1"
      },
      "outputs": [],
      "source": [
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkzWP1wvza7M"
      },
      "outputs": [],
      "source": [
        "# 설치된 패키지들을 import함수를 이용해서 불러오는 작업\n",
        "import torch\n",
        "assert torch.__version__.startswith(\"1.8\") \n",
        "import torchvision\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpBIn3wmzfHA"
      },
      "source": [
        "Detectron2에 필요한 데이터를 등록"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpPNLbp6ze0T"
      },
      "outputs": [],
      "source": [
        "# config하기 위해 필요한 패키지 불러오기 \n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "\n",
        "from detectron2.structures import BoxMode\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YHdwMvhzkB_"
      },
      "outputs": [],
      "source": [
        "# 라벨링한 json파일의 class 및 기본 정보를 얻는 def 함수  \n",
        "def get_data_dicts(directory, classes):\n",
        "    dataset_dicts = []\n",
        "    for filename in [file for file in os.listdir(directory) if file.endswith('.json')]:\n",
        "        json_file = os.path.join(directory, filename)\n",
        "        with open(json_file) as f:\n",
        "            img_anns = json.load(f)\n",
        "\n",
        "        record = {}\n",
        "        \n",
        "        filename = os.path.join(directory, img_anns[\"imagePath\"])\n",
        "        \n",
        "        record[\"file_name\"] = filename\n",
        "        #record[\"height\"] = 512\n",
        "        #record[\"width\"] = 384\n",
        "      \n",
        "        annos = img_anns[\"shapes\"]\n",
        "        objs = []\n",
        "        for anno in annos:\n",
        "            px = [a[0] for a in anno['points']] # x coord\n",
        "            py = [a[1] for a in anno['points']] # y coord\n",
        "            poly = [(x, y) for x, y in zip(px, py)] # segmentation을 위한 poly\n",
        "            poly = [p for x in poly for p in x]\n",
        "\n",
        "            obj = {\n",
        "                \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
        "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
        "                \"segmentation\": [poly],\n",
        "                \"category_id\": classes.index(anno['label']),\n",
        "                \"iscrowd\": 0\n",
        "            }\n",
        "            objs.append(obj)\n",
        "        record[\"annotations\"] = objs\n",
        "        dataset_dicts.append(record)\n",
        "    return dataset_dicts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2fmDLFWU_jh"
      },
      "outputs": [],
      "source": [
        "for d in [\"train\", \"test\"]:\n",
        "    DatasetCatalog.register(\"images_\" + d, lambda d=d: get_water_dicts(\"for seg/\" + d)) \n",
        "    MetadataCatalog.get(\"bimages_\" + d).set(thing_classes=[\"water\"])\n",
        "water_metadata = MetadataCatalog.get(\"water_train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2jLCgdW1YMK"
      },
      "outputs": [],
      "source": [
        "classes = ['water']\n",
        "\n",
        "data_path = '/content/drive/MyDrive/project_datacampus/dataset/for_seg/'\n",
        "\n",
        "for d in [\"train\", \"test\"]:\n",
        "    DatasetCatalog.register(\n",
        "        \"category_\" + d, \n",
        "        lambda d=d: get_data_dicts(data_path+d, classes)\n",
        "    )\n",
        "    MetadataCatalog.get(\"category_\" + d).set(thing_classes=classes)\n",
        "\n",
        "microcontroller_metadata = MetadataCatalog.get(\"category_train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-naA8zP41yGf"
      },
      "source": [
        "Detectron2의 Instance Segmentation 모델을 학습시키기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNmkVO421suj"
      },
      "outputs": [],
      "source": [
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultTrainer, DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import ColorMode, Visualizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgnsvrgJ13KC"
      },
      "outputs": [],
      "source": [
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"category_train\",)\n",
        "cfg.DATASETS.TEST = ()\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2 \n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 2000\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajBdJ6Re15L1"
      },
      "outputs": [],
      "source": [
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg) \n",
        "trainer.resume_or_load(resume=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwNTMVn81895"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-Dn_f4y1_il"
      },
      "source": [
        "train를 통해 나온 모델을 사용한 추론(Inference)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VooJ5_vk2A4m"
      },
      "outputs": [],
      "source": [
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"/content/drive/MyDrive/project_datacampus/dataset/for_seg/model_final_last.pth\")\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.75 \n",
        "cfg.DATASETS.TEST = (\"skin_test\", )\n",
        "predictor = DefaultPredictor(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hN50Unvd2Dr6"
      },
      "outputs": [],
      "source": [
        "test_dataset_dicts = get_data_dicts(data_path+'test', classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2ZpbfYW2Fd1"
      },
      "outputs": [],
      "source": [
        "for d in random.sample(test_dataset_dicts, 3):    \n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "    outputs = predictor(img)\n",
        "    v = Visualizer(img[:, :, ::-1],\n",
        "                   metadata=microcontroller_metadata, \n",
        "                   scale=0.8, \n",
        "                   instance_mode=ColorMode.IMAGE_BW \n",
        "    )\n",
        "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    plt.figure(figsize = (14, 10))\n",
        "    plt.imshow(cv2.cvtColor(v.get_image()[:, :, ::-1], cv2.COLOR_BGR2RGB))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcq1eBp_UJzX"
      },
      "outputs": [],
      "source": [
        "from detectron2.utils.visualizer import ColorMode\n",
        "from google.colab.patches import cv2_imshow\n",
        "dataset_dicts = get_data_dicts(\"/content/drive/MyDrive/project_datacampus/dataset/for_seg/test\",'water') \n",
        "for d in random.sample(dataset_dicts, 3):    \n",
        "    im = cv2.imread(d[\"file_name\"])\n",
        "    outputs = predictor(im)  \n",
        "    v = Visualizer(im[:, :, ::-1],\n",
        "                   metadata=water_metadata, \n",
        "                   scale=0.5, \n",
        "                   instance_mode=ColorMode.IMAGE_BW   \n",
        "    )\n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tensorboard를 통해 학습된 모델의 세부 성능 지표를 확인"
      ],
      "metadata": {
        "id": "9PCoNNRfk1h8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcvznMjBTfCZ"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFyGkismZcfx"
      },
      "source": [
        "이미지 파일을 이용한 면적 구하는 과정 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyMjYTCv7K4w"
      },
      "outputs": [],
      "source": [
        "# colab에 detectron2 폴더 생성\n",
        "!git clone https://github.com/facebookresearch/detectron2\n",
        "%cd /content/detectron2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9v5Bq1sYoUQ"
      },
      "outputs": [],
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "import os\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(\"/content/detectron2/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "cfg.DATASETS.TRAIN = (\"water\",)\n",
        "cfg.DATASETS.TEST = ()   \n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = \"/content/drive/MyDrive/project_datacampus/dataset/for_seg/model_final_last.pth\" \n",
        "cfg.SOLVER.BASE_LR = 0.025\n",
        "cfg.SOLVER.MAX_ITER = 2000   \n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   \n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # 1 classes (water)\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyfvKHJHY8hR"
      },
      "outputs": [],
      "source": [
        "cfg.MODEL.WEIGHTS = \"/content/drive/MyDrive/project_datacampus/dataset/for_seg/model_final_last.pth\" \n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.75  # 모델의 정확도 설정 파트 75%로 설정됨 \n",
        "cfg.DATASETS.TEST = (\"water\", )\n",
        "predictor = DefaultPredictor(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMUSoWKHZDp2"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/MyDrive/project_datacampus/dataset/for_seg/test/image_200.jpg\"\n",
        "im = cv2.imread(path)\n",
        "outputs = predictor(im)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJAX6gsZZOvh"
      },
      "outputs": [],
      "source": [
        "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "cv2_imshow(v.get_image()[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uA5CHrNDQ_t5"
      },
      "outputs": [],
      "source": [
        "mask_array = outputs[\"instances\"].pred_masks.cpu().numpy()\n",
        "num_instances = mask_array.shape[0]\n",
        "mask_array = np.moveaxis(mask_array, 0, -1)\n",
        "\n",
        "mask_array_instance = []\n",
        "output = np.zeros_like(im) #black\n",
        "\n",
        "#print('output',output)\n",
        "for i in range(num_instances):\n",
        "    mask_array_instance.append(mask_array[:, :, i:(i+1)])\n",
        "    # print(mask_array_instance)\n",
        "    output = np.where(mask_array_instance[i] == True, 255, output)\n",
        "\n",
        "cv2_imshow(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBn2bQrQ4kvO"
      },
      "outputs": [],
      "source": [
        "### 영상 읽기 \n",
        "img_raw = output.copy()\n",
        "### color 영상을 Grayscale 영상으로 변환\n",
        "img_gray = cv2.cvtColor(img_raw,cv2.COLOR_BGR2GRAY)\n",
        "### Otsu's thresholding\n",
        "_, img_binary = cv2.threshold(img_gray,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU) \n",
        "### 닫기 연산 (잡음제거)\n",
        "kernel = np.ones((3,3),np.uint8)\n",
        "img_morph = cv2.morphologyEx(img_binary,cv2.MORPH_CLOSE,kernel)\n",
        "### 컨투어 찾기 - 꼭짓점 좌표 cv2.findContours(src,mode,method,contours,hierarchy,offset)\n",
        "contours,hierarchy = cv2.findContours(img_morph,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
        "### 컨투어 그리기 -cv2.drawContours(img,contours,contourldx,color,thickness) contourldx = -1 모든 컨투어 :표시\n",
        "img_out = img_raw.copy()\n",
        "_=cv2.drawContours(img_out,contours,-1,(0,0,200),10) #-1로 고려 필요할듯 전체를 컨투어 해야하기에 \n",
        "\n",
        "region = 0\n",
        "for cnt in contours:\n",
        "    area = cv2.contourArea(cnt)\n",
        "    region = region + area\n",
        "AREA=round(region/(img_out.shape[0]*img_out.shape[1])*100,2)\n",
        "# AREA의 면적 퍼센트에 따라 글씨의 색상을 바꾸어서 경보단계를 표시 \n",
        "if AREA <= 10:\n",
        "  color = (255,0,0)\n",
        "elif AREA <= 20:\n",
        "  color = (0,255,255)\n",
        "else:\n",
        "  color = (0,0,255)\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "#color = (255, 0, 0)\n",
        "fontScale = img_out.shape[0]/200\n",
        "cv2.putText(img_out, f\"{AREA:.2f}%\", (20,50), font, fontScale, color,10) #사진의 크기마다 조절 혹은 전체 resize \n",
        "### 영상 출력 \n",
        "plt.imshow(cv2.cvtColor(img_out,cv2.COLOR_BGR2RGB))\n",
        "plt.axis('on') #on하면 눈금값도 나옴 \n",
        "plt.show"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "contour를 통해 segmentation면적 구하는 공식 "
      ],
      "metadata": {
        "id": "peOOlNxj0DDW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZwQqsrY_Hr_"
      },
      "outputs": [],
      "source": [
        "region = 0\n",
        "for cnt in contours:\n",
        "    area = cv2.contourArea(cnt)\n",
        "    region = region + area\n",
        "display(region)\n",
        "display(round(region/(img_out.shape[0]*img_out.shape[1])*100,2)) #반올림 처리 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RiFoPA1xF4A"
      },
      "source": [
        "영상 디텍팅\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0sHIK-b5j3t"
      },
      "outputs": [],
      "source": [
        "# 유튜브의 영상 확인\n",
        "from IPython.display import YouTubeVideo, display\n",
        "video = YouTubeVideo(\"EixdqWFs-Os\", width=500) # 5R3vOr0iyZI(도로침수) //mv5LtkXBdso(도로) //AsXMIBHxPLY(댐)\n",
        "display(video)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDNeLzMVQJiO"
      },
      "outputs": [],
      "source": [
        "# 유튜브에서 영상 다운로드\n",
        "!pip install youtube-dl\n",
        "!pip uninstall -y opencv-python opencv-contrib-python\n",
        "!apt install python3-opencv\n",
        "!youtube-dl https://www.youtube.com/watch?v=EixdqWFs-Os -f 22 -o video.mp4\n",
        "!ffmpeg -i video.mp4 -t 00:00:8 -c:v copy video-clip.mp4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJrSuMwN6NwW"
      },
      "outputs": [],
      "source": [
        "# 다운로드한 영상을 segmentation 진행 \n",
        "from detectron2.utils.video_visualizer import VideoVisualizer\n",
        "from detectron2.utils.visualizer import ColorMode, Visualizer\n",
        "import tqdm\n",
        "\n",
        "# 비디오의 속성 추출\n",
        "video = cv2.VideoCapture('video-clip.mp4') #유튜브가 아닌 다른 소스로 저장된 mp4도 넣으면 가능 \n",
        "width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "frames_per_second = video.get(cv2.CAP_PROP_FPS)\n",
        "num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "\n",
        "video_writer = cv2.VideoWriter('out.mp4', fourcc=cv2.VideoWriter_fourcc(*\"mp4v\"), fps=float(frames_per_second), frameSize=(width, height), isColor=True)\n",
        "\n",
        "v = VideoVisualizer(MetadataCatalog.get(cfg.DATASETS.TEST[0]), ColorMode.IMAGE)\n",
        "\n",
        "def runOnVideo(video, maxFrames):\n",
        "    \"\"\" Runs the predictor on every frame in the video (unless maxFrames is given),\n",
        "    and returns the frame with the predictions drawn.\n",
        "    \"\"\"\n",
        "\n",
        "    readFrames = 0\n",
        "    while True:\n",
        "        hasFrame, frame = video.read()\n",
        "        if not hasFrame:\n",
        "            break\n",
        "\n",
        "        # 프레임의 예측값 얻기\n",
        "        outputs = predictor(frame)\n",
        "\n",
        "        # 프레임의 컬러가 있도록 \n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        # instance segmentation를 통해 예측 결과를 표시\n",
        "        visualization = v.draw_instance_predictions(frame, outputs[\"instances\"].to(\"cpu\"))\n",
        "\n",
        "        # Matplotlib RGB 포맷을 OpenCV BGR 포맷으로 변경\n",
        "        visualization = cv2.cvtColor(visualization.get_image(), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        yield visualization\n",
        "\n",
        "        readFrames += 1\n",
        "        if readFrames > maxFrames:\n",
        "            break\n",
        "\n",
        "for visualization in tqdm.tqdm(runOnVideo(video, num_frames), total=num_frames):\n",
        "\n",
        "    video_writer.write(visualization)\n",
        "\n",
        "video.release()\n",
        "video_writer.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "vVBtRDuK68a0",
        "outputId": "7d3c5ad2-9e69-4e92-aee7-f4835ec647d9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6219f709-35c8-4dc5-aed2-54f148654bed\", \"out.mp4\", 63805927)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# segmentation하여 나온 결과 영상 저장 \n",
        "from google.colab import files\n",
        "files.download('/content/detectron2/out.mp4')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "저장된 비디오 캡쳐(10 or 30초)"
      ],
      "metadata": {
        "id": "LMeobxcGqj2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1초당 프레임 체크 \n",
        "filepath = '/content/drive/MyDrive/project_datacampus/dataset/for_seg/video/road_origin.mp4'\n",
        "video = cv2.VideoCapture(filepath)\n",
        "fps = video.get(cv2.CAP_PROP_FPS)\n",
        "print(fps)\n",
        "frame_count = video.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "display(frame_count)"
      ],
      "metadata": {
        "id": "7kiixfEsyEME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "비디오를 일정 프레임마다 캡쳐하여 이미지로 변경 "
      ],
      "metadata": {
        "id": "ExpW2-vQu2hL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# 영상의 의미지를 연속적으로 캡쳐할 수 있게 하는 class\n",
        "# 영상이 있는 경로\n",
        "vidcap = cv2.VideoCapture('/content/drive/MyDrive/project_datacampus/dataset/for_seg/video/road_origin.mp4') ####start from here\n",
        "\n",
        "count = 0\n",
        "\n",
        "while(vidcap.isOpened()):\n",
        "    ret, image = vidcap.read()\n",
        "    # 이미지 사이즈 960x540으로 변경\n",
        "    #image = cv2.resize(image, (960, 540)) #512,384\n",
        "    #image = cv2.rotate(image, cv2.ROTATE_180)\n",
        "     \n",
        "    saveme = open('/content/drive/MyDrive/project_datacampus/dataset/for_seg/road/cap/frame.txt', 'a') \n",
        "    # 일정시간마다 하나씩 이미지 캡쳐\n",
        "    if(int(vidcap.get(1)) % 29 == 0): \n",
        "        print('Saved frame number : ' + str(int(vidcap.get(1))))\n",
        "        print(str(int(vidcap.get(1))),file=saveme)\n",
        "        # 추출된 이미지가 저장되는 경로\n",
        "        cv2.imwrite(\"/content/drive/MyDrive/project_datacampus/dataset/for_seg/road/cap/frame%d.jpg\" % count, image)\n",
        "        #print('Saved frame%d.jpg' % count)\n",
        "        count += 1\n",
        "        if (vidcap.get(1)) == 1044:\n",
        "          break\n",
        "        \n",
        "vidcap.release() "
      ],
      "metadata": {
        "id": "VjNpvbYivbJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "캡쳐한 이미지를 segmentation한 이후 contour를 통해 면적 비율 구하기 "
      ],
      "metadata": {
        "id": "Gh1EO2aJurqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.config import get_cfg\n",
        "import os\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import glob\n",
        "\n",
        "#cfg = get_cfg()\n",
        "#cfg.merge_from_file(\"/content/detectron2/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "#cfg.DATASETS.TRAIN = (\"water\",)\n",
        "#cfg.DATASETS.TEST = ()   # no metrics implemented for this dataset\n",
        "#cfg.DATALOADER.NUM_WORKERS = 2\n",
        "#cfg.MODEL.WEIGHTS = \"/content/drive/MyDrive/project_datacampus/dataset/for_seg/model_final_last.pth\"  # initialize from model zoo 경로확인!!\n",
        "#cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "#cfg.SOLVER.BASE_LR = 0.025\n",
        "#cfg.SOLVER.MAX_ITER = 2000   # 300 iterations seems good enough, but you can certainly train longer\n",
        "#cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset\n",
        "#cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # 1 classes (person)\n",
        "\n",
        "#os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "#cfg.MODEL.WEIGHTS = \"/content/drive/MyDrive/K-Water/for seg/model_final_last.pth\" # 여기부분은 본인의 model이저장된 경로로 수정해줍니다.\n",
        "#cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.75  # set the testing threshold for this model\n",
        "#cfg.DATASETS.TEST = (\"water\", )\n",
        "#predictor = DefaultPredictor(cfg)\n",
        "\n",
        "file_path = '/content/drive/MyDrive/project_datacampus/dataset/for_seg/road/cap/'\n",
        "list_images = glob.glob(file_path + '*.jpg')\n",
        "j=0\n",
        "k=0\n",
        "for image in list_images:\n",
        "  im = Image.open(image)\n",
        "  im = np.array(im)\n",
        "  outputs = predictor(im)\n",
        "  v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "  v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "  cv2.imwrite('/content/drive/MyDrive/project_datacampus/dataset/for_seg/road/seg/frame%d.jpg'%j,v.get_image())\n",
        "  j=j+1\n",
        "  mask_array = outputs[\"instances\"].pred_masks.cpu().numpy()\n",
        "  num_instances = mask_array.shape[0]\n",
        "  mask_array = np.moveaxis(mask_array, 0, -1)\n",
        "\n",
        "  mask_array_instance = []\n",
        "  output = np.zeros_like(im) #black\n",
        "  for i in range(num_instances):\n",
        "    mask_array_instance.append(mask_array[:, :, i:(i+1)])\n",
        "    output = np.where(mask_array_instance[i] == True, 255, output)\n",
        "  img_raw = output.copy()\n",
        "  ### color 영상을 Grayscale 영상으로 변환\n",
        "  img_gray = cv2.cvtColor(img_raw,cv2.COLOR_BGR2GRAY)\n",
        "  ### Otsu's thresholding\n",
        "  _, img_binary = cv2.threshold(img_gray,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU) \n",
        "  ### 닫기 연산 (잡음제거)\n",
        "  kernel = np.ones((3,3),np.uint8)\n",
        "  img_morph = cv2.morphologyEx(img_binary,cv2.MORPH_CLOSE,kernel)\n",
        "  ### 컨투어 찾기 - 꼭짓점 좌표 cv2.findContours(src,mode,method,contours,hierarchy,offset)\n",
        "  contours,hierarchy = cv2.findContours(img_morph,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
        "  ### 컨투어 그리기 -cv2.drawContours(img,contours,contourldx,color,thickness) contourldx = -1 모든 컨투어 :표시\n",
        "  img_out = img_raw.copy()\n",
        "  _=cv2.drawContours(img_out,contours,-1,(0,0,200),10) \n",
        "\n",
        "  region = 0\n",
        "  for cnt in contours:\n",
        "    saveme = open('/content/drive/MyDrive/project_datacampus/dataset/for_seg/road/cap/output.txt', 'a') \n",
        "    area = cv2.contourArea(cnt)\n",
        "    region = region + area\n",
        "  AREA=round(region/(img_out.shape[0]*img_out.shape[1])*100,2)\n",
        "  # AREA의 면적 퍼센트에 따라 글씨의 색상을 바꾸어서 경보단계를 표시 \n",
        "  if AREA <= 30:\n",
        "    color = (255,0,0)\n",
        "  elif AREA <= 40:\n",
        "    color = (0,255,255)\n",
        "  else:\n",
        "    color = (0,0,255)\n",
        "  font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "  #color = (255, 0, 0)\n",
        "  fontScale = img_out.shape[0]/200\n",
        "  cv2.putText(img_out, f\"{AREA:.2f}%\", (20,150), font, fontScale, color,13) \n",
        "  display(AREA)\n",
        "  print(AREA, file=saveme)\n",
        "\n",
        "  #cv2_imshow(img_out[:, :, ::-1])\n",
        "  cv2.imwrite(\"/content/drive/MyDrive/project_datacampus/dataset/for_seg/road/cnt/frame%d.jpg\"%k,img_out)\n",
        "  k=k+1"
      ],
      "metadata": {
        "id": "FLc3n2fjYAS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "txt파일 두개 결합이후 plotting"
      ],
      "metadata": {
        "id": "zI4cAisun8DR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# txt형태의 파일을 csv형태의 파일로 로드\n",
        "df1 = pd.read_table('/content/drive/MyDrive/project_datacampus/dataset/for_seg/road/cap/frame.txt',sep='\\n',header=None,names=['fps'])\n",
        "df2 = pd.read_table('/content/drive/MyDrive/project_datacampus/dataset/for_seg/road/cap/output.txt',sep='\\n',header=None,names=['area'])\n",
        "# 두개의 파일을 하나로 합쳐주기\n",
        "df_result = pd.concat([df1,df2],axis=1)\n",
        "#df_result = df_result[:89]\n",
        "df_result\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# seaborn를 이용한 plotting\n",
        "fig, ax = plt.subplots(figsize=(13, 7)) \n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "plt.title('Segmentation Area Ratio', fontsize=17)\n",
        "sns.lineplot(df_result.fps, df_result.area)\n",
        "ax.set_xlabel('fps', size=13)\n",
        "ax.set_ylabel('AREA (%)', size=13)\n",
        "plt.axhline(y=30, color='yellow', linewidth=1)\n",
        "plt.axhline(y=40, color='orange', linewidth=1)\n",
        "plt.axhline(y=50, color='red', linewidth=1)\n",
        "ax.legend(['Ratio','alert 1','alert 2','alert 3'])\n",
        "plt.show() "
      ],
      "metadata": {
        "id": "AYh7b4Orn7wF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 강의 면적 수치에 대한 정보 \n",
        "display(df2.min())\n",
        "display(df2.max())\n",
        "display(df2.mean())"
      ],
      "metadata": {
        "id": "ivQCnjkbyGnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "참고 출처\n",
        "- <https://www.kaggle.com/code/ehabibrahim758/flood-segmentation-and-classification/notebook> \n",
        "\n",
        "- <https://github.com/facebookresearch/detectron2>\n",
        "\n",
        "- <https://www.geumriver.go.kr/html/sumun/river_movie_all.jsp>\n",
        "\n",
        "- <https://gilberttanner.com/blog/detectron2-train-a-instance-segmentation-model/>\n",
        "\n",
        "- <https://www.mdpi.com/2076-3417/11/20/9691/htm>"
      ],
      "metadata": {
        "id": "hvcZnWvF2Cf3"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}